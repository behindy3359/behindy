from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import os
import logging
from datetime import datetime
import uuid
from contextvars import ContextVar

from providers.llm_provider import LLMProviderFactory
from services.story_service import StoryService
from utils.rate_limiter import RateLimiter
from utils.api_auth import verify_internal_api_key, is_internal_request

from models.batch_models import BatchStoryRequest, BatchStoryResponse
from services.batch_story_service import BatchStoryService

INTERNAL_API_KEY = os.getenv("INTERNAL_API_KEY")
if not INTERNAL_API_KEY:
    raise RuntimeError("INTERNAL_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Request ID ì»¨í…ìŠ¤íŠ¸ ë³€ìˆ˜
request_id_var = ContextVar("request_id", default=None)

app = FastAPI(
    title="Behindy AI Server",
    description="ìŠ¤í† ë¦¬ ìƒì„± ì„œë²„",
    version="3.0.0"
)

ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "http://localhost:3000").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["Content-Type", "X-Internal-API-Key"],
)

logger.info(" FastAPI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”")

logger.info(" BatchStoryService ì´ˆê¸°í™” ì¤‘...")
batch_story_service = BatchStoryService()
logger.info(f" BatchStoryService ì´ˆê¸°í™” ì™„ë£Œ: {batch_story_service}")

logger.info(" RateLimiter ì´ˆê¸°í™” ì¤‘...")
rate_limiter = RateLimiter()
logger.info(f" RateLimiter ì´ˆê¸°í™” ì™„ë£Œ: {rate_limiter}")

# Provider ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì„¸ì…˜ ì¬ì‚¬ìš©ìš©)
_provider_instance = None

def get_provider_instance():
    """Get singleton provider instance for session reuse"""
    global _provider_instance
    if _provider_instance is None:
        _provider_instance = LLMProviderFactory.get_provider()
        logger.info(f"Created provider instance: {_provider_instance.get_provider_name()}")
    return _provider_instance

MAX_REQUEST_SIZE = int(os.getenv("MAX_REQUEST_SIZE", "1048576"))  # 1MB

@app.middleware("http")
async def add_request_id(request: Request, call_next):
    """Request ID ì¶”ì  ë¯¸ë“¤ì›¨ì–´"""
    request_id = str(uuid.uuid4())
    request_id_var.set(request_id)

    response = await call_next(request)
    response.headers["X-Request-ID"] = request_id

    return response

@app.middleware("http")
async def limit_request_size(request: Request, call_next):
    content_length = request.headers.get("content-length")

    if content_length and int(content_length) > MAX_REQUEST_SIZE:
        logger.warning(f" ìš”ì²­ í¬ê¸° ì´ˆê³¼: {content_length} > {MAX_REQUEST_SIZE}")
        return {"error": "ìš”ì²­ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤.", "status_code": 413}

    response = await call_next(request)
    return response

@app.middleware("http")
async def log_requests(request: Request, call_next):
    request_id = request_id_var.get()
    logger.info(f"[{request_id}] ë“¤ì–´ì˜¤ëŠ” ìš”ì²­: {request.method} {request.url}")
    logger.info(f"[{request_id}] í´ë¼ì´ì–¸íŠ¸ IP: {request.client.host}")
    logger.info(f"[{request_id}] í—¤ë”: {dict(request.headers)}")

    response = await call_next(request)

    logger.info(f"[{request_id}] ì‘ë‹µ ìƒíƒœ: {response.status_code}")
    return response


@app.on_event("startup")
async def startup_event():
    """Initialize provider on startup"""
    logger.info("Starting up AI server...")
    provider = get_provider_instance()
    logger.info(f"Provider ready: {provider.get_provider_name()}")

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    logger.info("Shutting down AI server...")
    global _provider_instance
    if _provider_instance and hasattr(_provider_instance, 'close'):
        await _provider_instance.close()
        logger.info("Provider session closed")

@app.get("/")
async def root():
    """ê¸°ë³¸ í—¬ìŠ¤ ì²´í¬"""
    provider = get_provider_instance()

    return {
        "message": "Behindy AI Server (Simplified)",
        "status": "healthy",
        "provider": provider.get_provider_name(),
        "timestamp": datetime.now().isoformat(),
        "version": "3.0.0",
        "endpoints": ["generate-complete-story", "health", "providers"]
    }

@app.get("/health")
async def health_check():
    """Enhanced health check with detailed system status"""
    try:
        provider = get_provider_instance()
        available_providers = LLMProviderFactory.get_available_providers()

        # Check provider connection health
        provider_health = "healthy"
        try:
            if hasattr(provider, '_session') and provider._session:
                if provider._session.closed:
                    provider_health = "session_closed"
                    logger.warning("Provider session is closed, will recreate on next request")
        except Exception as e:
            logger.error(f"Error checking provider health: {e}")
            provider_health = "unknown"

        health_status = {
            "status": "healthy",
            "current_provider": provider.get_provider_name(),
            "provider_health": provider_health,
            "available_providers": available_providers,
            "connection_pooling": {
                "enabled": hasattr(provider, '_session'),
                "session_active": hasattr(provider, '_session') and provider._session and not provider._session.closed
            },
            "rate_limiter": {
                "total_requests": rate_limiter.get_total_requests(),
                "status": "active"
            },
            "timestamp": datetime.now().isoformat(),
            "uptime_check": "ok",
            "version": "3.0.0"
        }

        return health_status

    except Exception as e:
        logger.error(f"Health check failed: {e}", exc_info=True)
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/providers")
async def get_providers_status():
    """Provider ìƒíƒœ í™•ì¸"""
    available_providers = LLMProviderFactory.get_available_providers()
    current_provider = LLMProviderFactory.get_provider()
    
    return {
        "current": current_provider.get_provider_name(),
        "available": available_providers,
        "settings": {
            "ai_provider": os.getenv("AI_PROVIDER", "mock"),
            "openai_model": os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            "claude_model": os.getenv("CLAUDE_MODEL", "claude-3-haiku"),
        }
    }

@app.post("/generate-complete-story", response_model=BatchStoryResponse)
async def generate_complete_story(request: BatchStoryRequest, http_request: Request):
    try:
        api_key = http_request.headers.get("X-Internal-API-Key")
        if api_key == INTERNAL_API_KEY:
            logger.info(" ë‚´ë¶€ API í‚¤ ì¸ì¦ ì„±ê³µ")
            request_mode = "BATCH"
        else:
            logger.info(" ì¼ë°˜ API í˜¸ì¶œ")
            request_mode = "PUBLIC"
        
        request_id = request_id_var.get()
        logger.info(f"[{request_id}] " + "=" * 40)

        if request_mode == "PUBLIC":
            client_ip = http_request.client.host
            logger.info(f"[{request_id}] Rate Limiting ì²´í¬ ì‹œì‘ (IP: {client_ip})")
            rate_limiter.check_rate_limit(client_ip)
            logger.info(f"[{request_id}] Rate Limiting í†µê³¼")
        else:
            logger.info(f"[{request_id}] Rate Limiting ê±´ë„ˆëœ€")

        response = await batch_story_service.generate_complete_story(request)

        logger.info(f"[{request_id}] ìŠ¤í† ë¦¬ ìƒì„± ì™„ë£Œ: {response.story_title}")
        logger.info(f"[{request_id}] " + "=" * 40)
        
        return response
        
    except HTTPException as e:
        logger.error(f" HTTPException ë°œìƒ:")
        logger.error(f"  ìƒíƒœì½”ë“œ: {e.status_code}")
        logger.error(f"  ìƒì„¸ ë‚´ìš©: {e.detail}")
        raise  # Rate limit ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
    except Exception as e:
        logger.error(f" ìŠ¤í† ë¦¬ ìƒì„± ì‹¤íŒ¨:")
        logger.error(f"  ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        logger.error(f"  ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
        logger.error(f"  ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:", exc_info=True)
        
        # ğŸ†• ì—ëŸ¬ ë°œìƒì‹œ fallback ì‘ë‹µ
        logger.warning("âš ï¸ ì—ëŸ¬ ë°œìƒìœ¼ë¡œ fallback ì‘ë‹µ ìƒì„±")
        return BatchStoryResponse(
            story_title=f"{request.station_name}ì—­ì˜ ì´ì•¼ê¸°",
            description=f"{request.station_name}ì—­ì—ì„œ ë²Œì–´ì§€ëŠ” í¥ë¯¸ì§„ì§„í•œ ëª¨í—˜",
            theme="ë¯¸ìŠ¤í„°ë¦¬",
            keywords=[request.station_name, f"{request.line_number}í˜¸ì„ ", "ì§€í•˜ì² "],
            pages=[
                BatchPageData(
                    content=f"{request.station_name}ì—­ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì¼ì´ ë²Œì–´ì§‘ë‹ˆë‹¤. ì–´ë–»ê²Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
                    options=[
                        BatchOptionData(
                            content="ì‹ ì¤‘í•˜ê²Œ í–‰ë™í•œë‹¤",
                            effect="sanity",
                            amount=2,
                            effect_preview="ì •ì‹ ë ¥ +2"
                        ),
                        BatchOptionData(
                            content="ë¹ ë¥´ê²Œ ëŒ€ì‘í•œë‹¤",
                            effect="health", 
                            amount=-1,
                            effect_preview="ì²´ë ¥ -1"
                        )
                    ]
                )
            ],
            estimated_length=1,
            difficulty="ë³´í†µ",
            station_name=request.station_name,
            line_number=request.line_number
        )

# ===== ê´€ë¦¬ ë° ë””ë²„ê¹… API =====

@app.post("/validate-story-structure")
async def validate_story_structure(validation_request: Dict[str, Any], http_request: Request):
    """ìŠ¤í† ë¦¬ êµ¬ì¡° ê²€ì¦ (ë‚´ë¶€ API)"""
    try:
        # ë‚´ë¶€ API í‚¤ ê²€ì¦
        api_key = http_request.headers.get("X-Internal-API-Key")
        if api_key != INTERNAL_API_KEY:
            raise HTTPException(status_code=403, detail="Unauthorized internal API access")
        
        logger.info("ìŠ¤í† ë¦¬ êµ¬ì¡° ê²€ì¦ ìš”ì²­")
        
        validation_result = await batch_story_service.validate_story_structure(
            validation_request.get("story_data", {})
        )
        
        return validation_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="ìŠ¤í† ë¦¬ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨")

@app.get("/batch/system-status")
async def get_batch_system_status(http_request: Request):
    """ë°°ì¹˜ ì‹œìŠ¤í…œ ìƒíƒœ (ë‚´ë¶€/ê³µê°œ API)"""
    try:
        # ë‚´ë¶€ API í‚¤ í™•ì¸ (ì„ íƒì )
        api_key = http_request.headers.get("X-Internal-API-Key")
        is_internal = api_key == INTERNAL_API_KEY
        
        provider = LLMProviderFactory.get_provider()
        available_providers = LLMProviderFactory.get_available_providers()
        
        status = {
            "ai_server_status": "healthy",
            "current_provider": provider.get_provider_name(),
            "available_providers": available_providers,
            "batch_service_ready": True,
            "rate_limit_status": {
                "total_requests": rate_limiter.get_total_requests(),
                "hit_rate": "N/A"
            },
            "timestamp": datetime.now().isoformat(),
            "simplified_mode": True,
            "version": "3.0.0"
        }
        
        # ë‚´ë¶€ ìš”ì²­ì¸ ê²½ìš° ë” ìƒì„¸í•œ ì •ë³´ ì œê³µ
        if is_internal:
            status["internal_mode"] = True
            status["api_endpoints"] = ["generate-complete-story"]
        
        return status
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨")

# ===== í…ŒìŠ¤íŠ¸ API =====

@app.post("/test-provider")
async def test_provider(test_request: Dict[str, Any]):
    """Provider í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸"""
    try:
        provider = LLMProviderFactory.get_provider()
        
        logger.info(" Provider í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info(f"  í˜„ì¬ Provider: {provider.get_provider_name()}")
        logger.info(f"  í…ŒìŠ¤íŠ¸ ìš”ì²­: {test_request}")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìŠ¤í† ë¦¬ ìƒì„± ìš”ì²­
        test_request_obj = BatchStoryRequest(
            station_name=test_request.get("station_name", "ê°•ë‚¨"),
            line_number=test_request.get("line_number", 2),
            character_health=80,
            character_sanity=80,
            story_type="TEST"
        )
        
        test_result = await batch_story_service.generate_complete_story(test_request_obj)
        
        return {
            "provider": provider.get_provider_name(),
            "status": "success",
            "test_result": {
                "title": test_result.story_title,
                "theme": test_result.theme,
                "pages": len(test_result.pages)
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Provider í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return {
            "provider": "unknown",
            "status": "failed",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ===== í™˜ê²½ ì„¤ì • API =====

@app.get("/config")
async def get_config():
    """í˜„ì¬ í™˜ê²½ ì„¤ì • í™•ì¸"""
    return {
        "ai_provider": os.getenv("AI_PROVIDER", "mock"),
        "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
        "claude_configured": bool(os.getenv("CLAUDE_API_KEY")),
        "request_limits": {
            "per_hour": os.getenv("REQUEST_LIMIT_PER_HOUR", "100"),
            "per_day": os.getenv("REQUEST_LIMIT_PER_DAY", "1000")
        },
        "simplified_mode": True,
        "active_endpoints": [
            "/generate-complete-story",
            "/health", 
            "/providers",
            "/batch/system-status"
        ]
    }

# ===== ì—ëŸ¬ í•¸ë“¤ëŸ¬ =====

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    logger.warning(f"HTTP ì˜¤ë¥˜: {exc.status_code} - {exc.detail}")
    return {
        "error": exc.detail,
        "status_code": exc.status_code,
        "timestamp": datetime.now().isoformat()
    }

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(f"ì¼ë°˜ ì˜¤ë¥˜: {str(exc)}")
    return {
        "error": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "status_code": 500,
        "timestamp": datetime.now().isoformat()
    }

# ===== ì„œë²„ ì‹¤í–‰ =====

if __name__ == "__main__":
    import uvicorn
    
    # ì‹œì‘ì‹œ Provider ìƒíƒœ ë¡œê¹…
    try:
        provider = LLMProviderFactory.get_provider()
        available = LLMProviderFactory.get_available_providers()
        
        logger.info("=" * 60)
        logger.info(" Behindy AI Server ì‹œì‘ (Simplified)")
        logger.info(f" í˜„ì¬ Provider: {provider.get_provider_name()}")
        logger.info(f" ì‚¬ìš© ê°€ëŠ¥í•œ Providers: {available}")
        logger.info(" í™œì„±í™”ëœ ì—”ë“œí¬ì¸íŠ¸:")
        logger.info("  - POST /generate-complete-story (í†µí•© ìŠ¤í† ë¦¬ ìƒì„±)")
        logger.info("  - GET  /health (í—¬ìŠ¤ì²´í¬)")
        logger.info("  - GET  /providers (Provider ìƒíƒœ)")
        logger.info("  - GET  /batch/system-status (ì‹œìŠ¤í…œ ìƒíƒœ)")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)