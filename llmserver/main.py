from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import os
import logging
from datetime import datetime

from providers.llm_provider import LLMProviderFactory
from services.story_service import StoryService
from models.request_models import StoryGenerationRequest, StoryContinueRequest
from models.response_models import StoryGenerationResponse, StoryContinueResponse, OptionData
from utils.rate_limiter import RateLimiter

from models.batch_models import BatchStoryRequest, BatchStoryResponse
from services.batch_story_service import BatchStoryService


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="Behindy AI Server",
    description="ì§€í•˜ì²  ìŠ¤í† ë¦¬ ìƒì„±ìš© ê²Œì´íŠ¸ì›¨ì´",
    version="2.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
story_service = StoryService()
rate_limiter = RateLimiter()

# ===== í—¬ìŠ¤ì²´í¬ ë° ìƒíƒœ =====

@app.get("/")
async def root():
    """ê¸°ë³¸ í—¬ìŠ¤ ì²´í¬"""
    provider = LLMProviderFactory.get_provider()
    
    return {
        "message": "Behindy AI Server (Enhanced)",
        "status": "healthy",
        "provider": provider.get_provider_name(),
        "timestamp": datetime.now().isoformat(),
        "version": "2.0.0"
    }

@app.get("/health")
async def health_check():
    """ìƒì„¸ í—¬ìŠ¤ ì²´í¬"""
    provider = LLMProviderFactory.get_provider()
    available_providers = LLMProviderFactory.get_available_providers()
    
    return {
        "status": "healthy",
        "current_provider": provider.get_provider_name(),
        "available_providers": available_providers,
        "supported_stations": len(story_service.get_supported_stations()),
        "total_requests": rate_limiter.get_total_requests(),
        "timestamp": datetime.now().isoformat()
    }

@app.get("/providers")
async def get_providers_status():
    """Provider ìƒíƒœ í™•ì¸"""
    available_providers = LLMProviderFactory.get_available_providers()
    current_provider = LLMProviderFactory.get_provider()
    
    return {
        "current": current_provider.get_provider_name(),
        "available": available_providers,
        "settings": {
            "ai_provider": os.getenv("AI_PROVIDER", "mock"),
            "openai_model": os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
            "claude_model": os.getenv("CLAUDE_MODEL", "claude-3-haiku"),
            "local_model": os.getenv("LOCAL_LLM_MODEL", "llama2:7b")
        }
    }

# ===== ìŠ¤í† ë¦¬ ìƒì„± API =====

@app.post("/generate-story", response_model=StoryGenerationResponse)
async def generate_story(request: StoryGenerationRequest, http_request: Request):
    """ìƒˆë¡œìš´ ìŠ¤í† ë¦¬ ìƒì„± (ì‹¤ì œ LLM ì‚¬ìš©)"""
    try:
        # Rate Limiting
        client_ip = http_request.client.host
        rate_limiter.check_rate_limit(client_ip)
        
        logger.info(f"ìŠ¤í† ë¦¬ ìƒì„± ìš”ì²­: {request.station_name} ({request.line_number}í˜¸ì„ ) - Provider: {LLMProviderFactory.get_provider().get_provider_name()}")
        
        # ìŠ¤í† ë¦¬ ìƒì„± (ì‹¤ì œ LLM ë˜ëŠ” Mock)
        response = await story_service.generate_story(request)
        
        logger.info(f"ìŠ¤í† ë¦¬ ìƒì„± ì™„ë£Œ: {response.story_title}")
        return response
        
    except HTTPException:
        raise  # Rate limit ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
    except Exception as e:
        logger.error(f"ìŠ¤í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìŠ¤í† ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/continue-story", response_model=StoryContinueResponse)
async def continue_story(request: StoryContinueRequest, http_request: Request):
    """ìŠ¤í† ë¦¬ ì§„í–‰ (ì‹¤ì œ LLM ì‚¬ìš©)"""
    try:
        # Rate Limiting
        client_ip = http_request.client.host
        rate_limiter.check_rate_limit(client_ip)
        
        logger.info(f"ìŠ¤í† ë¦¬ ì§„í–‰ ìš”ì²­: {request.station_name}, ì„ íƒ: {request.previous_choice}")
        
        # ìŠ¤í† ë¦¬ ì§„í–‰
        response = await story_service.continue_story(request)
        
        logger.info(f"ìŠ¤í† ë¦¬ ì§„í–‰ ì™„ë£Œ: {len(response.options)}ê°œ ì„ íƒì§€")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìŠ¤í† ë¦¬ ì§„í–‰ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìŠ¤í† ë¦¬ ì§„í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ===== ë©”íƒ€ë°ì´í„° API =====

@app.get("/stations")
async def get_supported_stations():
    """ì§€ì›ë˜ëŠ” ì—­ ëª©ë¡"""
    stations = story_service.get_supported_stations()
    return {
        "stations": stations,
        "total_count": len(stations),
        "supported_lines": [1, 2, 3, 4]
    }

@app.get("/popular-stations")
async def get_popular_stations():
    """ì¸ê¸° ì—­ í†µê³„"""
    popular = story_service.get_popular_stations()
    return {
        "popular_stations": popular,
        "total_requests": rate_limiter.get_total_requests()
    }

# ===== í…ŒìŠ¤íŠ¸ API =====

@app.post("/test-provider")
async def test_provider(test_request: Dict[str, Any]):
    """Provider í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸"""
    try:
        provider = LLMProviderFactory.get_provider()
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
        test_story = await provider.generate_story(
            "í…ŒìŠ¤íŠ¸ ìŠ¤í† ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.",
            station_name="ê°•ë‚¨",
            line_number=2,
            character_health=80,
            character_sanity=80
        )
        
        return {
            "provider": provider.get_provider_name(),
            "status": "success",
            "test_result": test_story,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Provider í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return {
            "provider": "unknown",
            "status": "failed",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ===== í™˜ê²½ ì„¤ì • API =====

@app.get("/config")
async def get_config():
    """í˜„ì¬ í™˜ê²½ ì„¤ì • í™•ì¸"""
    return {
        "ai_provider": os.getenv("AI_PROVIDER", "mock"),
        "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
        "claude_configured": bool(os.getenv("CLAUDE_API_KEY")),
        "local_llm_url": os.getenv("LOCAL_LLM_URL", "http://localhost:11434"),
        "request_limits": {
            "per_hour": os.getenv("REQUEST_LIMIT_PER_HOUR", "100"),
            "per_day": os.getenv("REQUEST_LIMIT_PER_DAY", "1000")
        }
    }

# ===== ì—ëŸ¬ í•¸ë“¤ëŸ¬ =====

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    logger.warning(f"HTTP ì˜¤ë¥˜: {exc.status_code} - {exc.detail}")
    return {
        "error": exc.detail,
        "status_code": exc.status_code,
        "timestamp": datetime.now().isoformat()
    }

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(f"ì¼ë°˜ ì˜¤ë¥˜: {str(exc)}")
    return {
        "error": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "status_code": 500,
        "timestamp": datetime.now().isoformat()
    }

# ===== ì„œë²„ ì‹¤í–‰ =====

if __name__ == "__main__":
    import uvicorn
    
    # ì‹œì‘ì‹œ Provider ìƒíƒœ ë¡œê¹…
    try:
        provider = LLMProviderFactory.get_provider()
        available = LLMProviderFactory.get_available_providers()
        
        logger.info("=" * 50)
        logger.info("ğŸš€ Behindy AI Server ì‹œì‘")
        logger.info(f"ğŸ“¡ í˜„ì¬ Provider: {provider.get_provider_name()}")
        logger.info(f"ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ Providers: {available}")
        logger.info(f"ğŸ¯ ì§€ì› ì—­ ê°œìˆ˜: {len(story_service.get_supported_stations())}")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì¶”ê°€
batch_story_service = BatchStoryService()

# ===== ë°°ì¹˜ ìŠ¤í† ë¦¬ ìƒì„± API (Spring Boot ì „ìš©) =====

@app.post("/generate-complete-story", response_model=BatchStoryResponse)
async def generate_complete_story(request: BatchStoryRequest, http_request: Request):
    """Spring Boot ë°°ì¹˜ ì‹œìŠ¤í…œìš© ì™„ì „í•œ ìŠ¤í† ë¦¬ ìƒì„±"""
    try:
        # ë‚´ë¶€ API í‚¤ ê²€ì¦
        api_key = http_request.headers.get("X-Internal-API-Key")
        if api_key != "behindy-internal-2024-secret-key":
            raise HTTPException(status_code=403, detail="Unauthorized internal API access")
        
        logger.info(f"ë°°ì¹˜ ìŠ¤í† ë¦¬ ìƒì„± ìš”ì²­: {request.station_name}ì—­ ({request.line_number}í˜¸ì„ )")
        
        # ì™„ì „í•œ ìŠ¤í† ë¦¬ ìƒì„±
        response = await batch_story_service.generate_complete_story(request)
        
        logger.info(f"ë°°ì¹˜ ìŠ¤í† ë¦¬ ìƒì„± ì™„ë£Œ: {response.story_title} ({len(response.pages)}í˜ì´ì§€)")
        return response
        
    except HTTPException:
        raise  # ì¸ì¦ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ìŠ¤í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìŠ¤í† ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/validate-story-structure")
async def validate_story_structure(validation_request: Dict[str, Any], http_request: Request):
    """ìŠ¤í† ë¦¬ êµ¬ì¡° ê²€ì¦ (ë‚´ë¶€ API)"""
    try:
        # ë‚´ë¶€ API í‚¤ ê²€ì¦
        api_key = http_request.headers.get("X-Internal-API-Key")
        if api_key != "behindy-internal-2024-secret-key":
            raise HTTPException(status_code=403, detail="Unauthorized internal API access")
        
        logger.info("ìŠ¤í† ë¦¬ êµ¬ì¡° ê²€ì¦ ìš”ì²­")
        
        validation_result = await batch_story_service.validate_story_structure(
            validation_request.get("story_data", {})
        )
        
        return validation_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/batch/system-status")
async def get_batch_system_status(http_request: Request):
    """ë°°ì¹˜ ì‹œìŠ¤í…œ ìƒíƒœ (ë‚´ë¶€ API)"""
    try:
        # ë‚´ë¶€ API í‚¤ ê²€ì¦ (ì„ íƒì )
        api_key = http_request.headers.get("X-Internal-API-Key")
        
        provider = LLMProviderFactory.get_provider()
        available_providers = LLMProviderFactory.get_available_providers()
        
        return {
            "ai_server_status": "healthy",
            "current_provider": provider.get_provider_name(),
            "available_providers": available_providers,
            "batch_service_ready": True,
            "supported_stations": len(story_service.get_supported_stations()),
            "quality_stats": story_service.get_quality_stats(),
            "rate_limit_status": {
                "total_requests": rate_limiter.get_total_requests(),
                "hit_rate": "N/A"
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")